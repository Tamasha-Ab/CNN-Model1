# -*- coding: utf-8 -*-
"""QCD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ujj_J7K6IZ_WM3M_nUqUdBjdTDCY10RF
"""

!pip install kaggle

# Manufacturing Defect Detection - Complete Training Pipeline
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import os
import json
from zipfile import ZipFile
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
import matplotlib.pyplot as plt
import shutil

# Install required packages
!pip install kaggle tensorflow scikit-learn pandas matplotlib

# Setup Kaggle credentials
try:
    kaggle_dictionary = json.load(open("kaggle.json"))
    os.environ["KAGGLE_USERNAME"] = kaggle_dictionary["username"]
    os.environ["KAGGLE_KEY"] = kaggle_dictionary["key"]
    print("Kaggle credentials loaded successfully")
except FileNotFoundError:
    print("kaggle.json not found. Please ensure Kaggle API credentials are set up.")

# Download dataset
!kaggle datasets download -d yidazhang07/bridge-cracks-image

!ls

# unzip the dataset file
with ZipFile("bridge-cracks-image.zip", "r") as zip_ref:
  zip_ref.extractall()

!ls

data = "/content/Bridge_Crack_Image/DBCC_Training_Data_Set"

base_path = '/content/Bridge_Crack_Image/DBCC_Training_Data_Set/train'
crack_dir = os.path.join(base_path, 'crack')
no_crack_dir = os.path.join(base_path, 'no_crack')

# Create folders
os.makedirs(crack_dir, exist_ok=True)
os.makedirs(no_crack_dir, exist_ok=True)

# Move images
for fname in os.listdir(base_path):
    fpath = os.path.join(base_path, fname)
    if os.path.isfile(fpath):
        if fname.lower().startswith('crack'):
            shutil.move(fpath, crack_dir)
        elif fname.lower().startswith('no'):
            shutil.move(fpath, no_crack_dir)

# Training data augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

# Validation and test data (no augmentation, only rescaling)
val_test_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

# Create generators
train_generator = train_datagen.flow_from_directory(
    os.path.join(data, "train"),
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='training',
    shuffle=True
)

validation_generator = val_test_datagen.flow_from_directory(
    os.path.join(data, "train"),
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='validation',
    shuffle=False
)

# Test generator (if test directory exists)
test_generator = None
test_dir = os.path.join(data, "test")
if os.path.exists(test_dir):
    test_generator = val_test_datagen.flow_from_directory(
        test_dir,
        target_size=(224, 224),
        batch_size=32,
        class_mode='binary',
        shuffle=False
    )

print(f"Training samples: {train_generator.samples}")
print(f"Validation samples: {validation_generator.samples}")
if test_generator:
    print(f"Test samples: {test_generator.samples}")

print(f"Class indices: {train_generator.class_indices}")

# Create model
print("\nCreating model...")

# Load pre-trained MobileNetV2
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)

# Freeze base model initially
base_model.trainable = False

# Add custom classification head
model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=[
        'accuracy',
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall'),
        tf.keras.metrics.AUC(name='auc')
    ]
)

print(f"Model created with {model.count_params():,} parameters")

# Create callbacks
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        verbose=1
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=8,
        min_lr=1e-7,
        verbose=1
    ),
    tf.keras.callbacks.ModelCheckpoint(
        'best_defect_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        save_weights_only=False,
        verbose=1
    )
]

# Train model
print("\nStarting initial training...")
history = model.fit(
    train_generator,
    epochs=5,
    validation_data=validation_generator,
    callbacks=callbacks,
    verbose=1
)

# Fine-tune model
print("\nStarting fine-tuning...")

# Unfreeze the top layers of the base model
base_model.trainable = True

# Fine-tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before fine_tune_at
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# Recompile with lower learning rate
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001/10),
    loss='binary_crossentropy',
    metrics=[
        'accuracy',
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall'),
        tf.keras.metrics.AUC(name='auc')
    ]
)

# Fine-tune training
fine_tune_history = model.fit(
    train_generator,
    epochs=5,
    validation_data=validation_generator,
    verbose=1
)

# Evaluate model
if test_generator:
    print("\nEvaluating model...")
    eval_generator = test_generator
else:
    print("\nEvaluating on validation data...")
    eval_generator = validation_generator

# Make predictions
print("Making predictions...")
predictions = model.predict(eval_generator, verbose=1)
y_pred = (predictions > 0.5).astype(int).flatten()
y_true = eval_generator.classes

# Calculate metrics
print("\n" + "="*50)
print("MODEL EVALUATION RESULTS")
print("="*50)

print("\nClassification Report:")
print(classification_report(y_true, y_pred,
                          target_names=['Non-Defective', 'Defective']))

print("\nConfusion Matrix:")
cm = confusion_matrix(y_true, y_pred)
print(cm)

# Calculate additional metrics
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
accuracy = accuracy_score(y_true, y_pred)

print(f"\nSummary Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# Plot training history
plt.figure(figsize=(15, 5))

# Plot accuracy
plt.subplot(1, 3, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
epochs = len(history.history['accuracy'])
plt.plot(range(epochs, epochs + len(fine_tune_history.history['accuracy'])),
        fine_tune_history.history['accuracy'], label='Fine-tune Training Accuracy')
plt.plot(range(epochs, epochs + len(fine_tune_history.history['val_accuracy'])),
        fine_tune_history.history['val_accuracy'], label='Fine-tune Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 3, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.plot(range(epochs, epochs + len(fine_tune_history.history['loss'])),
        fine_tune_history.history['loss'], label='Fine-tune Training Loss')
plt.plot(range(epochs, epochs + len(fine_tune_history.history['val_loss'])),
        fine_tune_history.history['val_loss'], label='Fine-tune Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot AUC
plt.subplot(1, 3, 3)
plt.plot(history.history['auc'], label='Training AUC')
plt.plot(history.history['val_auc'], label='Validation AUC')
plt.plot(range(epochs, epochs + len(fine_tune_history.history['auc'])),
        fine_tune_history.history['auc'], label='Fine-tune Training AUC')
plt.plot(range(epochs, epochs + len(fine_tune_history.history['val_auc'])),
        fine_tune_history.history['val_auc'], label='Fine-tune Validation AUC')
plt.title('Model AUC')
plt.xlabel('Epoch')
plt.ylabel('AUC')
plt.legend()

plt.tight_layout()
plt.show()

# Convert to TensorFlow.js
print("\nConverting model to TensorFlow.js format...")
!pip install tensorflowjs
import tensorflowjs as tfjs
tfjs.converters.save_keras_model(model, 'tfjs_model')
print("Model converted to TensorFlow.js format and saved to tfjs_model")

# Also save as regular Keras model
model.save('defect_detection_model.h5')
print("Model also saved as defect_detection_model.h5")